{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "crazy-jurisdiction",
   "metadata": {
    "papermill": {
     "duration": 0.011467,
     "end_time": "2022-08-02T06:46:35.453249",
     "exception": false,
     "start_time": "2022-08-02T06:46:35.441782",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fantastic-sarah",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T06:46:35.478907Z",
     "iopub.status.busy": "2022-08-02T06:46:35.477134Z",
     "iopub.status.idle": "2022-08-02T06:46:35.551317Z",
     "shell.execute_reply": "2022-08-02T06:46:35.551997Z",
     "shell.execute_reply.started": "2022-05-09T06:10:29.544546Z"
    },
    "papermill": {
     "duration": 0.089252,
     "end_time": "2022-08-02T06:46:35.552313",
     "exception": false,
     "start_time": "2022-08-02T06:46:35.463061",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input1</th>\n",
       "      <th>Input2</th>\n",
       "      <th>Input3</th>\n",
       "      <th>Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.659116</td>\n",
       "      <td>4.673695</td>\n",
       "      <td>1.231898</td>\n",
       "      <td>-1.010633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.172297</td>\n",
       "      <td>0.308095</td>\n",
       "      <td>6.197296</td>\n",
       "      <td>0.140169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.620350</td>\n",
       "      <td>3.752477</td>\n",
       "      <td>3.010036</td>\n",
       "      <td>0.014108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.269573</td>\n",
       "      <td>3.173038</td>\n",
       "      <td>6.019555</td>\n",
       "      <td>-0.016590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.946378</td>\n",
       "      <td>3.264015</td>\n",
       "      <td>2.245954</td>\n",
       "      <td>1.222183</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Input1    Input2    Input3    Output\n",
       "0  2.659116  4.673695  1.231898 -1.010633\n",
       "1  6.172297  0.308095  6.197296  0.140169\n",
       "2  1.620350  3.752477  3.010036  0.014108\n",
       "3  6.269573  3.173038  6.019555 -0.016590\n",
       "4  3.946378  3.264015  2.245954  1.222183"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_csv_path = '../input/toy-dataset-for-regression-and-uq/Data3_Train.csv'\n",
    "test_csv_path = '../input/toy-dataset-for-regression-and-uq/Data3_Test.csv'\n",
    "val_csv_path = '../input/toy-dataset-for-regression-and-uq/Data3_Val.csv'\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "val_df = pd.read_csv(val_csv_path)\n",
    "train_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smart-manufacturer",
   "metadata": {
    "papermill": {
     "duration": 0.010109,
     "end_time": "2022-08-02T06:46:35.572916",
     "exception": false,
     "start_time": "2022-08-02T06:46:35.562807",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ranging-poker",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T06:46:35.597258Z",
     "iopub.status.busy": "2022-08-02T06:46:35.596270Z",
     "iopub.status.idle": "2022-08-02T06:46:35.600361Z",
     "shell.execute_reply": "2022-08-02T06:46:35.600819Z",
     "shell.execute_reply.started": "2022-05-09T06:10:29.617524Z"
    },
    "papermill": {
     "duration": 0.017772,
     "end_time": "2022-08-02T06:46:35.601018",
     "exception": false,
     "start_time": "2022-08-02T06:46:35.583246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Similar_event_count = 100\n",
    "sensitivity_consideration = 1\n",
    "error_sensitivity_consideration = 1\n",
    "\n",
    "# Error-Signal Ratio\n",
    "ES_ratio = \"Yes\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-template",
   "metadata": {
    "papermill": {
     "duration": 0.009684,
     "end_time": "2022-08-02T06:46:35.620691",
     "exception": false,
     "start_time": "2022-08-02T06:46:35.611007",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Arranging and Normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sunset-hawaii",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T06:46:35.645202Z",
     "iopub.status.busy": "2022-08-02T06:46:35.644505Z",
     "iopub.status.idle": "2022-08-02T06:46:38.783504Z",
     "shell.execute_reply": "2022-08-02T06:46:38.784202Z",
     "shell.execute_reply.started": "2022-05-09T06:10:29.623849Z"
    },
    "papermill": {
     "duration": 3.153072,
     "end_time": "2022-08-02T06:46:38.784447",
     "exception": false,
     "start_time": "2022-08-02T06:46:35.631375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "tensor(1.4626) tensor(-1.4431) tensor([6.2822, 6.2817, 6.2823]) tensor([4.0563e-04, 6.2234e-05, 9.0123e-04])\n",
      "tensor([6.2818, 6.2816, 6.2814]) tensor(2.9057)\n"
     ]
    }
   ],
   "source": [
    "input_=[]\n",
    "output_=[]\n",
    "for row in train_df.iloc:\n",
    "    input_.append((row[0:len(row)-1]).astype(float))\n",
    "    output_.append(row[-1])\n",
    "    \n",
    "i_val=[]\n",
    "o_val=[]\n",
    "for row in val_df.iloc:\n",
    "    i_val.append((row[0:len(row)-1]).astype(float))\n",
    "    o_val.append(row[-1])\n",
    "    \n",
    "num_input = len(row)-1;\n",
    "print(num_input)\n",
    "\n",
    "####################################################\n",
    "#\n",
    "# This code is written with the help of the demo:\n",
    "# https://medium.com/@benjamin.phillips22/simple-regression-with-neural-networks-in-pytorch-313f06910379\n",
    "#\n",
    "####################################################\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import imageio\n",
    "\n",
    "torch.manual_seed(1)    # reproducible\n",
    "\n",
    "x = torch.tensor(input_).float()  \n",
    "y = torch.tensor(output_).float()   \n",
    "new_shape = (len(y), 1)\n",
    "y = y.view(new_shape)\n",
    "\n",
    "\n",
    "i_val = torch.tensor(i_val).float()  \n",
    "o_val = torch.tensor(o_val).float()   \n",
    "new_shape = (len(o_val), 1)\n",
    "o_val = o_val.view(new_shape)\n",
    "\n",
    "\n",
    "\n",
    "max_y = torch.max(y[:,0])\n",
    "min_y =torch.min(y[:,0])\n",
    "\n",
    "max_x = torch.max(x,dim=0)\n",
    "min_x = torch.min(x,dim=0)\n",
    "\n",
    "print(max_y, min_y, max_x.values, min_x.values)\n",
    "\n",
    "range_y = max_y - min_y\n",
    "range_x = max_x.values - min_x.values\n",
    "\n",
    "print(range_x, range_y)\n",
    "\n",
    "    #Normalizing\n",
    "x = (x - min_x.values)/range_x\n",
    "y = (y - min_y)/range_y\n",
    "\n",
    "    #Normalizing\n",
    "i_val = (i_val - min_x.values)/range_x\n",
    "o_val = (o_val - min_y)/range_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-oriental",
   "metadata": {
    "papermill": {
     "duration": 0.01518,
     "end_time": "2022-08-02T06:46:38.814808",
     "exception": false,
     "start_time": "2022-08-02T06:46:38.799628",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prediction NN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "perceived-native",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T06:46:38.844689Z",
     "iopub.status.busy": "2022-08-02T06:46:38.843711Z",
     "iopub.status.idle": "2022-08-02T06:47:27.885712Z",
     "shell.execute_reply": "2022-08-02T06:47:27.886435Z",
     "shell.execute_reply.started": "2022-05-09T06:10:32.486056Z"
    },
    "papermill": {
     "duration": 49.059988,
     "end_time": "2022-08-02T06:47:27.886705",
     "exception": false,
     "start_time": "2022-08-02T06:46:38.826717",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (hidden): Linear(in_features=3, out_features=500, bias=True)\n",
      "  (hidden2): Linear(in_features=500, out_features=500, bias=True)\n",
      "  (predict): Linear(in_features=500, out_features=1, bias=True)\n",
      ")\n",
      "Epoch [200/600], Loss: 0.0133, Minimum Loss 0.012997, Val Loss 0.015020  \n",
      "Epoch [400/600], Loss: 0.0060, Minimum Loss 0.005954, Val Loss 0.006948  \n",
      "Epoch [600/600], Loss: 0.0054, Minimum Loss 0.004941, Val Loss 0.005750  \n"
     ]
    }
   ],
   "source": [
    "# torch can only train on Variable, so convert them to Variable\n",
    "x, y = Variable(x), Variable(y)\n",
    " \n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(n_feature, n_hidden)   # hidden layer\n",
    "        self.hidden2 = torch.nn.Linear(n_hidden, n_hidden)   # hidden layer\n",
    "        self.predict = torch.nn.Linear(n_hidden, n_output)   # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden(x))      # activation function for hidden layer\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = self.predict(x)             # linear output\n",
    "        return x\n",
    "\n",
    "# use the same net as before      \n",
    "net = Net(n_feature=num_input, n_hidden=500, n_output=1)     # define the network\n",
    "print(net)  # net architecture\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.05)\n",
    "loss_func = torch.nn.MSELoss()  # this is for regression mean squared loss\n",
    "\n",
    "\n",
    "minimum_train_loss = 1e5 #High initial values\n",
    "minimum_val_loss = 1e5\n",
    "EPOCH = 600\n",
    "\n",
    "# start training\n",
    "for epoch in range(EPOCH):\n",
    "  \n",
    "    prediction = net(x)     # input x and predict based on x\n",
    "\n",
    "    loss = loss_func(prediction, y)     # must be (1. nn output, 2. target)\n",
    "\n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "    loss.backward()         # backpropagation, compute gradients\n",
    "    optimizer.step()        # apply gradients\n",
    "    \n",
    "    if loss<minimum_train_loss:\n",
    "        minimum_train_loss =loss\n",
    "        net_opt = net\n",
    "    \n",
    "    if epoch%200 == 199:\n",
    "      prediction = net_opt(i_val)\n",
    "      loss_val = loss_func(prediction, o_val)\n",
    "      if loss_val<minimum_val_loss:\n",
    "        minimum_val_loss = loss_val\n",
    "        net_opt_val = net_opt\n",
    "      print (\"Epoch [{}/{}], Loss: {:.4f}, Minimum Loss {:.6f}, Val Loss {:.6f}  \"  .format(epoch+1, EPOCH, loss, minimum_train_loss, minimum_val_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-induction",
   "metadata": {
    "papermill": {
     "duration": 0.011572,
     "end_time": "2022-08-02T06:47:27.911902",
     "exception": false,
     "start_time": "2022-08-02T06:47:27.900330",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Computing Test Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "graphic-oxygen",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T06:47:27.943653Z",
     "iopub.status.busy": "2022-08-02T06:47:27.942928Z",
     "iopub.status.idle": "2022-08-02T06:47:28.493875Z",
     "shell.execute_reply": "2022-08-02T06:47:28.494642Z",
     "shell.execute_reply.started": "2022-05-09T06:11:15.531431Z"
    },
    "papermill": {
     "duration": 0.571191,
     "end_time": "2022-08-02T06:47:28.494888",
     "exception": false,
     "start_time": "2022-08-02T06:47:27.923697",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Test Loss 0.0057005547\n",
      "Test Loss 0.04813022\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(test_csv_path)\n",
    "\n",
    "i_test=[]\n",
    "o_test=[]\n",
    "for row in test_df.iloc:\n",
    "    i_test.append((row[0:len(row)-1]).astype(float))\n",
    "    o_test.append(row[-1])\n",
    "\n",
    "\n",
    "   \n",
    "i_test, o_test = Variable(torch.tensor(i_test)).float(), Variable(torch.tensor(o_test).float())\n",
    "new_shape = (len(o_test), 1)\n",
    "o_test = o_test.view(new_shape)\n",
    "\n",
    "    #Normalizing\n",
    "i_test = (i_test - min_x.values)/range_x\n",
    "o_test = (o_test - min_y)/range_y\n",
    "\n",
    "prediction = net_opt_val(i_test)\n",
    "loss_test = loss_func(prediction, o_test)\n",
    "\n",
    "print(\"Normalized Test Loss\",loss_test.detach().numpy())\n",
    "\n",
    "loss_test = loss_test*range_y*range_y # As the loss function returns MSE\n",
    "\n",
    "print(\"Test Loss\",loss_test.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agricultural-essence",
   "metadata": {
    "papermill": {
     "duration": 0.011781,
     "end_time": "2022-08-02T06:47:28.519448",
     "exception": false,
     "start_time": "2022-08-02T06:47:28.507667",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# NN for Predicting Heteroscedastic Absolute Error of Previous NN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "precious-renaissance",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T06:47:28.554074Z",
     "iopub.status.busy": "2022-08-02T06:47:28.553327Z",
     "iopub.status.idle": "2022-08-02T06:49:25.858262Z",
     "shell.execute_reply": "2022-08-02T06:49:25.858806Z"
    },
    "papermill": {
     "duration": 117.32782,
     "end_time": "2022-08-02T06:49:25.859330",
     "exception": false,
     "start_time": "2022-08-02T06:47:28.531510",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (hidden): Linear(in_features=3, out_features=500, bias=True)\n",
      "  (hidden2): Linear(in_features=500, out_features=500, bias=True)\n",
      "  (predict): Linear(in_features=500, out_features=1, bias=True)\n",
      ")\n",
      "Error to Signal ratio: tensor(0.0319)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/loss.py:446: UserWarning: Using a target size (torch.Size([5000])) that is different to the input size (torch.Size([5000, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/600], Loss: 0.0020, Minimum Loss 0.002019, Val Loss 0.250923  \n",
      "Epoch [400/600], Loss: 0.0020, Minimum Loss 0.002016, Val Loss 0.250923  \n",
      "Epoch [600/600], Loss: 0.0020, Minimum Loss 0.002016, Val Loss 0.250923  \n"
     ]
    }
   ],
   "source": [
    "y_e = torch.zeros(len(x), dtype=torch.float) # initializing a variable of that size\n",
    "\n",
    "for iter1 in range(len(x)):\n",
    "    current_input = x[iter1,:];\n",
    "    current_output = net_opt_val(current_input);\n",
    "    y_e[iter1] = torch.abs(current_output - y[iter1]) \n",
    "        \n",
    "x, y_e = Variable(x), Variable(y_e)\n",
    "net_e = Net(n_feature=num_input, n_hidden=500, n_output=1)     # define the network\n",
    "print(net_e)  # net architecture\n",
    "\n",
    "# Error to Signal ratio\n",
    "ratio_e_s = 0.5 #default\n",
    "if ES_ratio == \"Yes\":\n",
    "    ratio_e_s = torch.var(y_e)/torch.var(y)\n",
    "\n",
    "    \n",
    "print('Error to Signal ratio:', ratio_e_s)\n",
    "\n",
    "optimizer = torch.optim.Adam(net_e.parameters(), lr=0.05)\n",
    "\n",
    "minimum_train_loss = 1e5 #High initial values\n",
    "minimum_val_loss = 1e5\n",
    "EPOCH = 600\n",
    "\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "  \n",
    "    prediction = net_e(x)     # input x and predict based on x\n",
    "\n",
    "    loss = loss_func(prediction, y_e)     # must be (1. nn output, 2. target)\n",
    "\n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "    loss.backward()         # backpropagation, compute gradients\n",
    "    optimizer.step()        # apply gradients\n",
    "    \n",
    "    if loss<minimum_train_loss:\n",
    "        minimum_train_loss =loss\n",
    "        net_opt = net_e\n",
    "    \n",
    "    if epoch%200 == 199:\n",
    "      prediction = net_opt(i_val)\n",
    "      loss_val = loss_func(prediction, o_val)\n",
    "      if loss_val<minimum_val_loss:\n",
    "        minimum_val_loss = loss_val\n",
    "        net_opt_val_e = net_opt\n",
    "      print (\"Epoch [{}/{}], Loss: {:.4f}, Minimum Loss {:.6f}, Val Loss {:.6f}  \"  .format(epoch+1, EPOCH, loss, minimum_train_loss, minimum_val_loss))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painful-yugoslavia",
   "metadata": {
    "papermill": {
     "duration": 0.013536,
     "end_time": "2022-08-02T06:49:25.887816",
     "exception": false,
     "start_time": "2022-08-02T06:49:25.874280",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Minimim deviation based similarity finding for Uncertainty Bounds\n",
    "\n",
    "   Aim: To achieve all indexes of similar samples\n",
    "\n",
    "   Steps: \n",
    "\n",
    "1. Itertion over all Input combinations.\n",
    "\n",
    "2. Find all deviation by indexes \n",
    "   (sensitivity can be considered during the consideration of deviation)\n",
    "\n",
    "3. Save top 100 similar events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "incomplete-serial",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T06:49:25.931889Z",
     "iopub.status.busy": "2022-08-02T06:49:25.931171Z",
     "iopub.status.idle": "2022-08-02T07:10:42.452793Z",
     "shell.execute_reply": "2022-08-02T07:10:42.453357Z"
    },
    "papermill": {
     "duration": 1276.551838,
     "end_time": "2022-08-02T07:10:42.453568",
     "exception": false,
     "start_time": "2022-08-02T06:49:25.901730",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0 %  Complete\n",
      "22.0 %  Complete\n",
      "42.0 %  Complete\n",
      "62.0 %  Complete\n",
      "82.0 %  Complete\n",
      "100% Complete in 17m 14s\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pickle as pkl\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "max_dev_index = np.zeros(shape=(len(x),2))\n",
    "Similarity = np.zeros(shape=(len(x),Similar_event_count+1))\n",
    "sensitivity = torch.ones(len(max_x.values), dtype=torch.float64) # just initializing a variable of that size\n",
    "error_sensitivity = torch.ones(len(max_x.values), dtype=torch.float64)\n",
    "iter_debug = 0\n",
    "since = time.time()\n",
    "\n",
    "for iter1 in range(len(x)):\n",
    "    current_input = x[iter1,:];\n",
    "    \n",
    "    # While considering only Sensitivity\n",
    "    if sensitivity_consideration == 1 and error_sensitivity_consideration ==0:\n",
    "        current_output = net_opt_val(current_input)\n",
    "        \n",
    "       # Determining Sensitivity near input[iter1]\n",
    "        for iter2 in range(len(current_input)): \n",
    "            current_input[iter2] = current_input[iter2] + range_x[iter2]/1e4\n",
    "            \n",
    "            sensitivity[iter2] = torch.abs(current_output - net_opt_val(current_input))\n",
    "            \n",
    "            current_input[iter2] = current_input[iter2] - range_x[iter2]/1e4\n",
    "        \n",
    "        sensitivity = sensitivity/torch.max(sensitivity)\n",
    "        #The highest sensitivity is considered one\n",
    "    \n",
    "    if sensitivity_consideration == 1 and error_sensitivity_consideration ==1:\n",
    "        current_output = net_opt_val(current_input)\n",
    "        current_error = net_opt_val_e(current_input)\n",
    "        # ratio, ratio_e_s= 0.5 while giving equal concentration to error and signal\n",
    "        \n",
    "       # Determining Sensitivity and Error Sensitivity near input[iter1]\n",
    "        for iter2 in range(len(current_input)): \n",
    "            current_input[iter2] = current_input[iter2] + range_x[iter2]/1e4\n",
    "            \n",
    "            sensitivity[iter2] = torch.abs(current_output - net_opt_val(current_input))\n",
    "            error_sensitivity[iter2] = torch.abs(current_error - net_opt_val_e(current_input))\n",
    "            \n",
    "            current_input[iter2] = current_input[iter2] - range_x[iter2]/1e4\n",
    "        \n",
    "        sensitivity = sensitivity/torch.max(sensitivity)*(1-ratio_e_s)\n",
    "        error_sensitivity = error_sensitivity/torch.max(error_sensitivity)* ratio_e_s\n",
    "        sensitivity = sensitivity + error_sensitivity\n",
    "        \n",
    "        #The highest sensitivity is considered half for each, then adding\n",
    "        \n",
    "    \n",
    "    for iter2 in range(len(x)):\n",
    "        max_dev_index[iter2][0] = torch.max(sensitivity*torch.abs(current_input-x[iter2,:])/range_x).detach().numpy()\n",
    "        max_dev_index[iter2][1] = iter2\n",
    "        \n",
    "    sort_dev_index = max_dev_index[np.argsort(max_dev_index[:, 0])]\n",
    "    Similarity_threshold = sort_dev_index[Similar_event_count][0]\n",
    "    matched_indexes = sort_dev_index[0:Similar_event_count,1]\n",
    "    \n",
    "    Similarity[iter1,0] = Similarity_threshold\n",
    "    Similarity[iter1,1:(Similar_event_count+1)] = matched_indexes\n",
    "    \n",
    "    if iter1%1000==100:   \n",
    "        print(iter1/len(x)*100, '%' '  Complete')\n",
    "        time_elapsed = time.time() - since\n",
    "        # Observing progres in Command Window\n",
    "        # May Take about hours to find all similar events for entire training data\n",
    "\n",
    "print('100% Complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))    \n",
    "\n",
    "fileName = './' + 'Similarity'\n",
    "fileObject = open(fileName, 'wb')\n",
    "\n",
    "pkl.dump(Similarity, fileObject)\n",
    "fileObject.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1455.12064,
   "end_time": "2022-08-02T07:10:44.486115",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-08-02T06:46:29.365475",
   "version": "2.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
